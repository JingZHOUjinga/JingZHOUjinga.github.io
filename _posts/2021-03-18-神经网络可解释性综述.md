---
layout:     post
title:      论文笔记|A Survey on Neural Network Interpretability
subtitle:   神经网络可解释性综述
date:       2021-03-18
author:     Jinga
header-img: img/grey.jpg
catalog: true
tags:
    - Neural Machine Translation
    - Natural Language Processing

---
# A Survey on Neural Network Interpretability

* [1.论文信息](#1)
* [2.论文摘要与简介](#2)
* [3.分类法](#3)
* [4.可解释性的测试](#4)
* [5.讨论](#5)
* [6.结论](#6)

<h2 id="1">1.论文信息</h2>

**作者:** Yu Zhang, Peter Tiňo, Aleš Leonardis, Ke Tang

**链接:** [https://arxiv.org/abs/2012.14261](https://arxiv.org/abs/2012.14261)  

这是清华大学计算机系与智能产业研究院的机器翻译研究团队发表的关于神经机器翻译技术的最新综述论文[<sup>1</sup>](#refer-anchor-1)  

清华大学智能产业研究院（Institute for AI Industry Research, Tsinghua University，英文简称AIR）是面向第四次工业革命的国际化、智能化、产业化的应用研究机构。AIR的使命是利用人工智能技术赋能产业升级、推动社会进步。通过大学与企业创新双引擎，突破人工智能核心技术，培养智能产业领军人才，推动智能产业跨越式发展。[<sup>2</sup>](#refer-anchor-2)

<h2 id="2">2.论文摘要与简介</h2>  

**Abstract**  

We  first  clarify  the  definition of interpretability as it has been used in many different contexts.  

Then  we  elaborate  on  the  importance  of  interpretability  and propose  a  novel  taxonomy  organized  along  three  dimensions: type of engagement (passive vs. active interpretation approaches),the  type  of  explanation,  and  the  focus  (from  local  to  global interpretability).  This  taxonomy  provides  a  meaningful  3D  view of  distribution  of  papers  from  the  relevant  literature  as  two of  the  dimensions  are  not  simply  categorical  but  allow  ordinal subcategories.   

Finally, we summarize the existing interpretability evaluation   methods   and   suggest   possible   research   directions inspired  by  our  new  taxonomy.  

**INTRODUCTION**  

These observations suggest that even though DNNs can achieve superior performance on many tasks, their underlying mechanisms may be very different from those of humans’ and have not yet been well-understood.  

**An (Extended) Definition of Interpretability**  

Interpretability is  the  ability  to  provide explanations in understandable terms to a human.  

We note that some studies distinguish between interpretability and explainability(or understandability,comprehensibility,transparency,human-simulatability etc.) In this paper we do not emphasize the subtle differences among those terms.  

**The Importance of Interpretability**  

However, a clearly organized exposition of such argumentation is missing. We summarize the arguments for the importance of interpretability into three groups.  
**1)High Reliability Requirement:**  

Interpretability  is  not  always  needed  but  it  is  important for  some  prediction  systems  that  are  required  to  be  highly reliable because an error may cause catastrophic results (e.g.,human lives, heavy financial loss).  

**2)  Ethical and Legal Requirement:**  

A first requirement is to avoid algorithmic discrimination.  

Deep  neural  networks  have  also  been  used  for  new  drug discovery and design  
 
Another legal requirement of interpretability is the "right to explanation"  

**3)  Scientific  Usage:**  

Deep  neural  networks  are  becoming powerful  tools  in  scientific  research  fields  where  the  data may  have  complex  intrinsic  pattern.  

**Related Work and Contributions**  

There  have  already  been  attempts  to  summarize  the  techniques for neural network interpretability. However, most of them only provide basic categorization or enumeration, without a clear taxonomy.   

This survey has the following contributions:  

We  make  a  further  step  towards  the  definition  of  interpretability on the basis of reference [15]. In this definition,we emphasize the type(or format)of explanations(e.g,rule  forms,  including  both  decision  trees  and  decisionrule  sets).  

We analyse the real needs for interpretability and summarize them into 3 groups: interpretability as an important component  in  systems  that  should  be  highly-reliable,ethical or legal requirements, and interpretability providing tools to enhance knowledge in the relevant science fields.  

We propose a new taxonomy comprising three dimensions(passive vs. active approaches, the format of explanations,and  local-semilocal-global  interpretability)  

<h2 id="3">3.分类法</h2>

The first  dimension is  categorical  and  has  two  possible  values,passive interpretation and active interpretability intervention.It divides the existing approaches according to whether they require to change the network architecture or the optimization process.   

our second dimension is type/format of explanation. By inspecting various kinds of explanations produced by different approaches, we can observe differences in how explicit they are.  We recognize four major types of explanations here,logic rules,hidden semantics,attribution and explanations  by  examples,  listed  in  order  of decreasing explanatory power.  

The  last  dimension,  from  local  to  global  interpretability(w.r.t.  the  input  space),  has  become  very  common  in  recent papers  

![taxonomy.png](/img/20210318taxonomy.png)

![table2.png](/img/20210318table2.png)

![table3.png](/img/20210318table3.png)
 
<h2 id="4">4.可解释性的测试</h2>  

For logic rules and decision trees, the size of the extracted rule model is often used as a criterion  

Network Dissection [18] quantifies the interpretability of hidden units by  calculating  their  matchiness  to  certain  concept  

As  for the hidden unit visualization approaches, there is no a good measurement yet.  

For attribution approaches, their explanations are saliency maps/masks (or feature importance etc. according to  the  specific  task)   

<h2 id="5">5.讨论</h2>   

Passive (post-hoc) methods have been widely studied because they can be applied in a relatively straightforward  manner  to  most  existing  networks.  

Active  (interpretability  intervention)  methods  put  forward ideas  about  how  a  network  should  be  optimized  to  gain interpretability  

As  for  the  second  dimension,  the  format  of  explanations,logical  rules  are  the  most  clear  (do  not  need  further  human interpretation)  

Hidden semantics essentially explain a subpart of a network,with  most  work  developed  in  the  computer  vision  field.  

Attribution  is  very  suitable  for  explaining  individual  inputs. 

Explaining by providing an example has the lowest (the most implicit) explanatory power.  

As for the last dimension, local explanations are more useful when we care more about every single prediction (e.g., a creditor insurance risk assessment)  

For some research fields, such as  genomics  and  astronomy,  global  explanations  are  more preferred as they may reveal some general “knowledge”.  

<h2 id="6">6.结论</h2>  

First,  we  have  discussed  the definition of interpretability and stressed the importance of the format of explanations and domain knowledge/representations.  

Then,  by  reviewing  the  previous literature,  we  summarized  3  essential  reasons  why  interpretability  is  important:  the  requirement  of  high  reliability systems, ethical/legal requirements and knowledge finding forscience.  
After that, we introduced a novel taxonomy for the existing network interpretation methods.    

From the perspective of the new taxonomy, there are still several possible research directions in the interpretability research  
First, the active interpretability intervention approaches are underexplored  
Another  important  research  direction  may  be  how  to  better  incorporate  domain  knowledge  in  the  networks.   





##### 参考:
<div id="refer-anchor-1"></div>
- [1] [如何解决神经机器翻译三大关键性问题？清华团队发表NMT最新技术综述](https://mp.weixin.qq.com/s/Ykx9qmRtYUN4DlFIBiN1MA)
<div id="refer-anchor-2"></div>
- [2] [我们是清华大学智能产业研究院](https://zhuanlan.zhihu.com/p/344166340)
<div id="refer-anchor-3"></div>